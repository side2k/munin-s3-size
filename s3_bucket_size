#!/usr/bin/python
from datetime import datetime
import subprocess
import os, sys, stat
import re

CACHE_PATH_DEFAULT = '/tmp/munin_s3_bucket_size.cache'
CACHE_MAX_LIFE = 3600 #seconds

def get_file_list():
    process = subprocess.Popen(['s3cmd', 'la', '-r'], stdout=subprocess.PIPE)
    return process.stdout.readlines()
    
def parse_list(lines):
    result = []
    for line in lines:
        if not line.strip():
            continue
        fields = [field for field in line.strip().split(' ') if field]
        result.append({
            'timestamp':datetime.strptime(
                '%s %s' % (fields[0], fields[1]),
                '%Y-%m-%d %H:%M'),
            'size':int(fields[2]),
            'path':fields[3]})
    return result
    
def get_bucket_name(path):
    (bucket_name, _, _) = path.replace('s3://', '', 1).partition('/')
    return bucket_name
    
def aggregate_results(data):
    result = {}
    for element in data:
        bucket_name = get_bucket_name(element['path'])
        if bucket_name in result:
            result[bucket_name] += element['size']
        else:
            result[bucket_name] = element['size']
    return result
    
def get_cached_data(cache_path = CACHE_PATH_DEFAULT):
    cache_valid = False
    if os.path.exists(cache_path):
        file_stats = os.stat(cache_path)
        cache_update = datetime.fromtimestamp(file_stats.st_mtime)
        update_delta = datetime.now() - cache_update
        cache_valid = \
            (update_delta.days * 86400 +update_delta.seconds) < CACHE_MAX_LIFE
        
    if not cache_valid:
        s3_list = get_file_list()
        cache_file = open(cache_path, 'w')
        cache_file.writelines(s3_list)
        cache_file.close()
        os.chmod(cache_path, stat.S_IRUSR | stat.S_IWUSR)
    else:
        cache_file = open(cache_path, 'r')
        s3_list = cache_file.readlines()
        cache_file.close()
        
    return parse_list(s3_list)

def normalize_name(name):
    normal_first = re.sub(r'^[^A-Za-z_]', r'_', name)
    return re.sub(r'[^A-Za-z0-9_]', r'_', normal_first)
   

args_started = False    
for arg in sys.argv:
    if arg == __file__:
        args_started = True
        continue
    switch = arg.lower()
    if switch == 'config':
        print 'graph_category Disk'
        print 'graph_title Amazon S3 storage usage per bucket'
        print 'graph_vlabel bytes'
        bucket_sizes = aggregate_results(get_cached_data())
        for bucket, size in bucket_sizes.iteritems():
            print '%s.label %s' % (normalize_name(bucket), bucket)
            
        sys.exit()
        
bucket_sizes = aggregate_results(get_cached_data())
for bucket, size in bucket_sizes.iteritems():
    print '%s.value %d' % (normalize_name(bucket), size)
    